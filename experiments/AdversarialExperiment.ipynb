{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-Lasso: Adversarial Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_lasso.data_splits import read_train_test_splits, read_baseline_splits\n",
    "from llm_lasso.task_specific_lasso.llm_lasso import *\n",
    "from llm_lasso.task_specific_lasso.plotting import plot_llm_lasso_result, plot_heatmap\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up Lung Cancer Splits\n",
    "See **`LungCancerExperiment.ipynb`** and run the first few cells.\n",
    "\n",
    "## Step 2: Command Line Portion\n",
    "Run the following in your command line\n",
    "```\n",
    "./shell_scripts/Lung_TCGA/adversarial/adversarial_gene_names.sh\n",
    "\n",
    "./shell_scripts/Lung_TCGA/adversarial/llm_score.sh\n",
    "\n",
    "./shell_scripts/Lung_TCGA/adversarial/llm_lasso_penalties.sh\n",
    "```\n",
    "\n",
    "**Note**: you need an OMIM API key to generate adversarial genenames, because it uses the OMIM API to check that the adversarial genenames don't already exist.\n",
    "\n",
    "## Step 3: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LLMLassoExperimentConfig(\n",
    "    folds_cv=5, # number of cross-validation folds\n",
    "    regression=False,\n",
    "    score_type=PenaltyType.PF, # We have penalty factors from the LLM,\n",
    "                               # not importance scores.\n",
    "    max_imp_power=1,\n",
    "    lambda_min_ratio=0.001, # Lasso parameter,\n",
    "    n_threads=8, # number of threads to use for computation\n",
    "    run_pure_lasso_after=10,\n",
    "    lasso_downstream_l2=True,\n",
    "    cross_val_metric=CrossValMetric.ERROR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 10\n",
    "DATASET=\"Lung_TCGA\"\n",
    "splits = read_train_test_splits(f\"../data/splits/{DATASET}\", N_SPLITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../data/adversarial/Lung_TCGA/new_genenames.pkl\", \"rb\") as f:\n",
    "    genenames = pickle.load(f)\n",
    "genename_mapping = {}\n",
    "for (old, new) in zip(splits[0].x_train.columns, genenames):\n",
    "    genename_mapping[old] = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N_SPLITS):\n",
    "    splits[i].x_train.columns = genenames\n",
    "    splits[i].x_test.columns = genenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_baseline = read_baseline_splits(f\"../data/baselines/{DATASET}\", n_splits=N_SPLITS, n_features=49)\n",
    "for key in feature_baseline:\n",
    "    for i in range(N_SPLITS):\n",
    "        feature_baseline[key][i] = [genename_mapping[x] for x in feature_baseline[key][i]]\n",
    "\n",
    "with open(f\"../data/adversarial/llm-score/Lung_TCGA/trial_scores_llm_score.json\") as f:\n",
    "    llm_score = json.load(f)\n",
    "features = splits[0].x_train.columns\n",
    "scores = np.mean(np.array([scores[\"scores\"] for scores in llm_score]), axis=0)\n",
    "llm_score_features = features[np.argsort(-scores)].tolist()\n",
    "feature_baseline[\"llm_score\"] = [llm_score_features] * N_SPLITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines = run_downstream_baselines_for_splits(\n",
    "    splits=splits,\n",
    "    feature_baseline=feature_baseline,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = run_lasso_baseline_for_splits(\n",
    "    splits=splits,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty_list={\n",
    "    \"plain\": np.array(\n",
    "        np.load(\"../data/adversarial/llm-lasso/Lung_TCGA/final_scores_plain.pkl\", allow_pickle=True)\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_lasso = run_llm_lasso_cv_for_splits(\n",
    "    splits=splits,\n",
    "    scores=penalty_list,\n",
    "    config=config,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_to_plot = [df[df[\"n_features\"] > 0] for df in [lasso, baselines, llm_lasso]]\n",
    "plot_llm_lasso_result(\n",
    "    dataframes_to_plot,\n",
    "    plot_error_bars=False,\n",
    "    test_error_y_lim=(0.04, 0.1),\n",
    "    auroc_y_lim=(0.95, 0.99),\n",
    "    x_lim=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-lasso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
