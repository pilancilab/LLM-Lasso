{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-Lasso: Large-Scale Lung Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_lasso.data_splits import read_train_test_splits, read_baseline_splits, save_train_test_splits\n",
    "from llm_lasso.task_specific_lasso.llm_lasso import *\n",
    "from llm_lasso.task_specific_lasso.plotting import plot_llm_lasso_result, plot_heatmap\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"../data/Lung_TCGA/expression.csv\")\n",
    "with open(\"../data/Lung_TCGA/labels.txt\", \"r\") as f:\n",
    "    y = pd.Series([\n",
    "        0 if line.strip() == \"\\\"LUAD\\\"\" else (1 if line.strip() == \"\\\"LUSC\\\"\" else 2) \\\n",
    "            for line in f.readlines()\n",
    "    ])\n",
    "assert not np.any(y == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_train_test_splits(X, y, \"../data/splits/Lung_TCGA\", balanced=True, n_splits=10, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Command Line Portion\n",
    "\n",
    "Run the following in your command line\n",
    "```\n",
    "./shell_scripts/Lung_TCGA/step_02_baselines.sh\n",
    "\n",
    "./shell_scripts/Lung_TCGA/step_03_llm_score_baseline.sh\n",
    "```\n",
    "\n",
    "### Penalties\n",
    "Scripts for running LLM-Lasso, are available in `shell_scripts/Lung_TCGA`. Note that setting up OMIM RAG is a bit time-intensive, but you can run plain LLM-Lasso without setting up OMIM RAG.\n",
    "\n",
    "For setting up OMIM RAG, refer to **`examples/omim_rag_tutorial.ipynb`**.\n",
    "\n",
    "## Step 3: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in splits\n",
    "N_SPLITS = 10\n",
    "splits = read_train_test_splits(\"../data/splits/Lung_TCGA\", N_SPLITS)\n",
    "n_features = splits[0].x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in LLM-Lasso Penalties\n",
    "\n",
    "with open(\"../data/Lung_TCGA/trial_scores_RAG.json\") as f:\n",
    "    trial_scores_rag = np.array([x[\"scores\"][0] for x in json.load(f)]) + 2\n",
    "penalties_rag = trial_scores_rag.mean(axis=0)\n",
    "\n",
    "with open(\"../data/Lung_TCGA/trial_scores_plain.json\") as f:\n",
    "    trial_scores_plain = np.array([x[\"scores\"][0] for x in json.load(f)]) + 2\n",
    "penalties_plain = trial_scores_plain.mean(axis=0)\n",
    "\n",
    "penalty_list={\n",
    "    \"plain\": penalties_plain,\n",
    "    \"rag\": penalties_rag,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in baseline features\n",
    "feature_baseline = read_baseline_splits(\n",
    "    \"../data/baselines/Lung_TCGA\", n_splits=N_SPLITS, n_features=49)\n",
    "\n",
    "with open(\"../data/llm-score/Lung_TCGA/llmselect_selected_features.json\", \"r\") as f:\n",
    "    llm_select_genes = json.load(f)[f\"{50}\"]\n",
    "\n",
    "feature_baseline[\"llm_score\"] = [llm_select_genes] * N_SPLITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LLMLassoExperimentConfig(\n",
    "    folds_cv=5, # number of cross-validation folds\n",
    "    regression=False,\n",
    "    score_type=PenaltyType.PF, # We have penalty factors from the LLM,\n",
    "                               # not importance scores.\n",
    "    max_imp_power=1,\n",
    "    lambda_min_ratio=0.001, # Lasso parameter,\n",
    "    n_threads=8, # number of threads to use for computation\n",
    "    run_pure_lasso_after=10,\n",
    "    lasso_downstream_l2=True,\n",
    "    cross_val_metric=CrossValMetric.ERROR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines = run_downstream_baselines_for_splits(\n",
    "    splits=splits,\n",
    "    feature_baseline=feature_baseline,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = run_lasso_baseline_for_splits(\n",
    "    splits=splits,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_lasso = run_llm_lasso_cv_for_splits(\n",
    "    splits=splits,\n",
    "    scores=penalty_list,\n",
    "    config=config,\n",
    "    score_trial_list={\n",
    "        \"plain\": trial_scores_plain,\n",
    "        \"rag\": trial_scores_rag\n",
    "    },\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_to_plot = [df[df[\"n_features\"] > 0] for df in [lasso,llm_lasso]]\n",
    "plot_llm_lasso_result(\n",
    "    dataframes_to_plot,\n",
    "    bolded_methods=[\"1/imp - rag\"],\n",
    "    plot_error_bars=False,\n",
    "    test_error_y_lim=(0.04, 0.081),\n",
    "    auroc_y_lim=(0.95, 0.99),\n",
    "    x_lim=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Win Ratio Bar Plot\n",
    "Direct comparison between RAG LLM-Lasso and Lasso in terms of how many points LLM-Lasso is strictly better than Lasso and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FEAT = 30\n",
    "all_results = pd.concat([\n",
    "    lasso, llm_lasso[llm_lasso[\"method_model\"] == \"1/imp - rag\"]\n",
    "], ignore_index=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method_model in all_results[\"method_model\"].unique():\n",
    "    for split in range(N_SPLITS):\n",
    "        prev_row = None\n",
    "        for nfeat in range(N_FEAT+1):\n",
    "            row = all_results[\n",
    "                np.bitwise_and(\n",
    "                    all_results[\"method_model\"] == method_model,\n",
    "                    np.bitwise_and(\n",
    "                        all_results[\"split\"] == split,\n",
    "                        all_results[\"n_features\"] == nfeat))\n",
    "                ]\n",
    "            if row.shape[0] == 1:\n",
    "                prev_row = row.copy()\n",
    "            elif row.shape[0] == 0:\n",
    "                if prev_row is not None:\n",
    "                    prev_row[\"n_features\"] = nfeat\n",
    "                    all_results = pd.concat([all_results, prev_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = all_results[all_results[\"n_features\"] <= N_FEAT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_counts = pd.DataFrame()\n",
    "\n",
    "for s in range(10):\n",
    "    split = all_results[all_results[\"split\"] == s]\n",
    "    split_reversed = split.iloc[::-1].reset_index(drop=True)\n",
    "    best_methods = (\n",
    "        split.loc[\n",
    "            split.groupby('n_features')['test_error'].idxmin()\n",
    "        ][['n_features', 'method_model', 'test_error']]\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    best_methods_rev = (\n",
    "        split_reversed.loc[\n",
    "            split_reversed.groupby('n_features')['test_error'].idxmin()\n",
    "        ][['n_features', 'method_model', 'test_error']]\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    best_methods = best_methods[best_methods[\"method_model\"] == best_methods_rev[\"method_model\"]]\n",
    "    # print(best_methods)\n",
    "    df = best_methods['method_model'].value_counts().reset_index() \n",
    "    df[\"split\"] = s\n",
    "    method_counts = pd.concat([df, method_counts], axis=0).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = method_counts.groupby(\"method_model\").agg(\n",
    "    mean=(\"count\", \"mean\"),\n",
    "    sd=(\"count\", \"std\")\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot_x = \"LUAD vs. LUSC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "our_mean = dataframe[dataframe[\"method_model\"] == \"1/imp - rag\"][\"mean\"].tolist()[0] /  N_FEAT\n",
    "lasso_mean = dataframe[dataframe[\"method_model\"] == \"Lasso\"][\"mean\"].tolist()[0] / N_FEAT\n",
    "our_sd = dataframe[dataframe[\"method_model\"] == \"1/imp - rag\"][\"sd\"].tolist()[0] / N_FEAT \n",
    "lasso_sd = dataframe[dataframe[\"method_model\"] == \"Lasso\"][\"sd\"].tolist()[0] / N_FEAT\n",
    "\n",
    "# positions of the bars on the x-axis\n",
    "x = np.arange(1)\n",
    "width = 0.35  # width of the bars\n",
    "\n",
    "# Create the figure and axes object\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plt.grid(zorder=0)\n",
    "\n",
    "# Plot bars for group 1\n",
    "rects1 = ax.bar(\n",
    "    x - width/2, [our_mean], width,\n",
    "    label='RAG LLM-Lasso',\n",
    "    color=\"#FE6100\",\n",
    "    alpha=0.8,\n",
    "    edgecolor=\"black\",\n",
    "    zorder=3\n",
    ")\n",
    "\n",
    "# Plot bars for group 2\n",
    "rects2 = ax.bar(\n",
    "    x + width/2, [lasso_mean], width,\n",
    "    label='Lasso',\n",
    "    color=\"#aaaaaa\",\n",
    "    alpha=0.8,\n",
    "    edgecolor=\"black\",\n",
    "    zorder=3\n",
    ")\n",
    "\n",
    "ax.errorbar(\n",
    "    x=x - width/2,\n",
    "    y=[our_mean],\n",
    "    yerr=[our_sd],\n",
    "    fmt='none',\n",
    "    c='black',\n",
    "    capsize=5,\n",
    "    zorder=5\n",
    ")\n",
    "ax.errorbar(\n",
    "    x=x + width/2,\n",
    "    y=[lasso_mean],\n",
    "    yerr=[lasso_sd],\n",
    "    fmt='none',\n",
    "    c='black',\n",
    "    capsize=5,\n",
    "    zorder=5\n",
    ")\n",
    "\n",
    "# Add some text for labels, title and axes ticks\n",
    "ax.set_title(f'Win Ratio Over First {N_FEAT} Features', fontdict={\"size\": 22})\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([barplot_x])\n",
    "ax.legend(fontsize=16, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tick_params(axis='both', labelsize=14) \n",
    "# plt.box(False)\n",
    "\n",
    "# fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-lasso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
